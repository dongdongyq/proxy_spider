1、上个爬虫爬取了西刺代理网站的ip信息，但是只能爬取一部分的信息，那如何爬取到所有的代理ip信息呢？

2、解决方案就是发请求时添加代理。

3、要添加一个代理，为了节省时间，首先先验证代理ip是否可用，其次当一个代理ip不可用时换下个可用ip。

4、这就需要建立一个代理池。将可用代理保存在列表里，根据返回结果状态码或异常更换ip。

5、第一步：读取上次爬虫爬取下来的代理ip信息   read_proxy()
   第二步：验证一个代理是否可用    verify_proxy()
   第三步：将可用代理另存为一个文件    save_available_proxy()
   ......
   重复一二三步，直到把所有的代理IP都验证完。为了提高效率，使用多线程来验证代理IP。

6、执行完proxy_pool.py后，可用代理IP会保存到文件。
   对proxy_spider.py进行改进，对其返回状态码进行判断，不等于200的更换代理继续爬取，直到爬取成功或无可用代理。

7、使用多线程时要注意，在io操作时（如写操作），要保证写入文件的数据不出现问题，
   需要给写操作添加线程锁，保证写操作是一个线程一个线程执行。
   if self.lock.acquire():

       写入文件程序
       self.lock.release()